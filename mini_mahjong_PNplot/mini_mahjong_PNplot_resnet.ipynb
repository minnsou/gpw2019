{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mini_mahjong_PNplot_resnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZWBR9WoZHQL",
        "colab_type": "text"
      },
      "source": [
        "### 必要な麻雀関連の関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3hyR93qZRwc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "random_seed = 34\n",
        "np.random.seed(random_seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWU-IyrnYt3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "def is_valid(seq, l=4): # 生成された組み合わせが手牌として妥当かどうかを判断する関数　tuple(seq)の一つ一つが一つの状態(手牌)に対応している\n",
        "    counts = defaultdict(lambda: 0)\n",
        "    for i in range(0, len(seq)):\n",
        "        if i + 1 < len(seq) and seq[i] > seq[i + 1]: # 前半の条件はiが一番最後以外は常に成立、後半の条件は昇順に整列するための条件\n",
        "            return False\n",
        "        counts[seq[i]] += 1\n",
        "        if (counts[seq[i]] > l): return False # 牌の上限枚数を超えたらFalse\n",
        "    return True\n",
        "\n",
        "import itertools\n",
        "def number_state_slow(n,m,l): # 全ての手牌の組み合わせの数を出力する関数\n",
        "    count = 0\n",
        "    for seq in itertools.product(range(n), repeat = m): # 直積を作る関数, n=9 m=5 なら 9 ** 5 回繰り返す　\n",
        "        if is_valid(seq,l):\n",
        "            count += 1\n",
        "            #print(list(seq))\n",
        "    return count\n",
        "    \n",
        "def generate_all_l(n, m, l=4): # 全ての手牌の組み合わせをタプルで出力する関数\n",
        "    gen_list = []\n",
        "    for seq in itertools.product(range(n), repeat = m):\n",
        "        if is_valid(seq, l):\n",
        "            gen_list.append(seq)\n",
        "    return gen_list\n",
        "\n",
        "def states_to_hist(state_list, n): # 手牌(state)を、牌種ごとの枚数のリスト(長さn)に変換する関数\n",
        "    hist_list = []\n",
        "    for state in state_list:\n",
        "        #print(state)\n",
        "        ret = [0] * n # ret = [0,0,...,0]\n",
        "        for c in state:\n",
        "            ret[c] += 1\n",
        "        hist_list.append(ret)\n",
        "    return hist_list\n",
        "\n",
        "def hand_to_prob_and_state(hand, state_nml, n, m, l=4): # ある手牌(hand)における、1枚ツモる時の遷移確率(prob)と手牌(state)のindexのタプルを出す関数\n",
        "    #print(state_nml)\n",
        "    ret = [l] * n  #  残り枚数を表すリスト\n",
        "    for h in hand:\n",
        "        ret[h] -= 1\n",
        "    yama_sum = n * l - (m - 1)\n",
        "    state_list = []\n",
        "    for i in range(n):\n",
        "        if ret[i] == 0: \n",
        "            continue\n",
        "        prob = ret[i] / yama_sum # 遷移確率\n",
        "        state = tuple(sorted(list(hand) + [i])) # 遷移後の手牌\n",
        "        #print(state)\n",
        "        state_index = state_nml.index(state) # 遷移後の手牌のindex\n",
        "        #print(state_index)\n",
        "        state_list.append((prob, state_index))\n",
        "    return state_list\n",
        "\n",
        "def state_to_hand(state): # ある手牌stateに遷移できるhandを出力する関数\n",
        "    return list(set(tuple(state[:i] + state[i+1:]) for i in range(len(state)))) # i番目の要素を取り除く\n",
        "\n",
        "def is_win_sub(hist, two, three):\n",
        "    if any(x < 0 for x in hist):\n",
        "        return False # この行を消したかったら、順子判定のところで手牌の枚数が負になるものを弾いておく\n",
        "    if two == 0 and three == 0:\n",
        "        return True\n",
        "    i = next(i for i, x in enumerate(hist) if x > 0) # histの中でx>０を満たす最小のindexを持ってくる\n",
        "    if two > 0 and hist[i] >= 2 and is_win_sub([x - 2 if i == j else x for j, x in enumerate(hist)], two - 1, three): # 雀頭\n",
        "        return True\n",
        "    if three > 0 and hist[i] >= 3 and is_win_sub([x - 3 if i == j else x for j, x in enumerate(hist)], two, three - 1): # 刻子\n",
        "        return True\n",
        "    if three > 0 and i + 2 < len(hist) and is_win_sub([x -1 if i <= j <= i + 2 else x for j, x in enumerate(hist)], two, three - 1): # 順子\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def is_win_main(hist):\n",
        "    n_two = 1 if sum(hist) % 3 == 2 else 0\n",
        "    n_three = sum(hist) // 3\n",
        "    return is_win_sub(hist, n_two, n_three)\n",
        "\n",
        "def value_iteration(n, m, l, gamma):\n",
        "    state_nml = generate_all_l(n, m, l)\n",
        "    hand_nml = generate_all_l(n, m-1, l)\n",
        "    hist_nml = states_to_hist(state_nml, n)\n",
        "    is_win_nml = [is_win_main(hist) for hist in hist_nml]\n",
        "    h2ps_nml = [hand_to_prob_and_state(hand, state_nml, n, m, l) for hand in hand_nml]\n",
        "    s2h_nml = [[hand_nml.index(hand) for hand in state_to_hand(state)] for state in state_nml]\n",
        "    value_hand = [0] * len(hand_nml)\n",
        "    n_hand = len(hand_nml)\n",
        "    value_state = [1 if is_win_nml[i] else 0 for i in range(len(state_nml))] # あがっていればvalueは1、いなければ0\n",
        "    n_state = len(state_nml)\n",
        "    theta = 1e-6\n",
        "    while True:\n",
        "        print('iteration')\n",
        "        delta = 0\n",
        "        for i in range(n_hand):\n",
        "            old_v = value_hand[i]\n",
        "            value_hand[i] = sum(p * value_state[n] for (p, n) in h2ps_nml[i])\n",
        "            delta = max(delta, abs(old_v - value_hand[i]))\n",
        "        if delta < theta: break\n",
        "        for i in range(n_state):\n",
        "            if is_win_nml[i]: continue\n",
        "            value_state[i] = max(gamma * value_hand[n] for n in s2h_nml[i])\n",
        "    return value_hand\n",
        "  \n",
        "def one_hot_vector1(hands, n): # 手牌の中の牌一つ一つをone-hotにした(手牌１つがn * m-1の行列に対応)\n",
        "    results = np.zeros((len(hands), n, len(hands[0])))\n",
        "    for i in range(len(hands)):\n",
        "        for j, hand_i in enumerate(hands[i]):\n",
        "            results[i][hand_i][j] = 1\n",
        "    return results\n",
        "\n",
        "def one_hot_vector2(hists, n, l=4): # histをそのままone-hotにした(手牌１つがn * l + 1の行列に対応)\n",
        "    results = np.zeros((len(hists), n, l + 1))\n",
        "    for i in range(len(hists)):\n",
        "        for j, hist_i in enumerate(hists[i]):\n",
        "            results[i][j][hist_i] = 1\n",
        "    return results\n",
        "\n",
        "def one_hot_vector3(hists, n, l=4): # 上に近いけど、持ってる枚数より小さい数も1で埋めた(手牌１つがn * lの行列に対応)\n",
        "    results = np.zeros((len(hists), n, l))\n",
        "    for i in range(len(hists)):\n",
        "        for j, hist_i in enumerate(hists[i]):\n",
        "            if hist_i == 0:\n",
        "                continue\n",
        "            else:\n",
        "                results[i][j][:hist_i] = 1\n",
        "    return results\n",
        "\n",
        "def state_to_hist(state, n): # 手牌(state)を、牌種ごとの枚数のリスト(長さn)に変換する関数\n",
        "    hist = [0] * n # hist = [0,0,...,0]\n",
        "    for c in state:\n",
        "        hist[c] += 1\n",
        "    return hist\n",
        "\n",
        "# stateとその時にvalueが最大となる捨て牌のタプルを入れたリスト max_value_discard_list = [((0, 0, 0, 0, 1), {0}), ((0, 0, 0, 0, 2), {0}), ... ,((7, 8, 8, 8, 8), {8})]\n",
        "# state_nmlのうち、あがり形を抜いたもの discard_state_nml = [(0, 0, 0, 0, 1), (0, 0, 0, 0, 2), ..., (7, 8, 8, 8, 8)]\n",
        "def states_to_max_value_list(state_nml, hand_nml, value_hand_nml, n, m, l=4):\n",
        "    max_value_list = []\n",
        "    discard_state_nml = []\n",
        "    hist_nml = states_to_hist(state_nml, n)\n",
        "    for i, hist in enumerate(hist_nml):        \n",
        "        if is_win_main(hist):\n",
        "            continue # あがっているstateの時は何も入れない\n",
        "        else:\n",
        "            max_value = 0\n",
        "            max_p = []\n",
        "            for j in range(m):\n",
        "                state = state_nml[i]\n",
        "                hand = state[:j] + state[j+1:]\n",
        "                ind = hand_nml.index(tuple(hand))\n",
        "                hand_val = value_hand_nml[ind]\n",
        "                if max_value < hand_val:\n",
        "                    max_p = {state[j]}\n",
        "                    max_value = hand_val\n",
        "                elif round(max_value, 5) == round(hand_val, 5): # 小数点以下5桁まで同じなら同じとみなす\n",
        "                    max_p.add(state[j])\n",
        "            discard_state_nml.append(state_nml[i])\n",
        "            max_value_list.append(tuple((state_nml[i], max_p)))\n",
        "    return max_value_list, discard_state_nml # 正直discard_hist_nmlを出す方が早い\n",
        "\n",
        "# 各stateにおいて、出力してほしい捨て牌の確率分布を出力する\n",
        "def discard_ans_prob_vector(max_value_discard_list, n, m, l):\n",
        "    discard_vector = []\n",
        "    for i, discard in max_value_discard_list:\n",
        "        v = [0] * n\n",
        "        num = len(discard)\n",
        "        for p in discard:\n",
        "            v[p] = 1 / num # 答えの数で割った値を教師とする  こうしないと学習がうまくいかない?\n",
        "            #v[p] = 1\n",
        "        discard_vector.append(v)\n",
        "    return discard_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDbkEK8pZfTc",
        "colab_type": "text"
      },
      "source": [
        "### 捨て牌ベクトルの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEh8WQREYx1-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 725
        },
        "outputId": "aabce11b-de67-4276-95b2-f4df59405cbc"
      },
      "source": [
        "n = 9\n",
        "m = 5\n",
        "l = 4\n",
        "\n",
        "value_hand_nml = value_iteration(n, m, l, 0.9)\n",
        "state_nml = generate_all_l(n, m, l)\n",
        "hand_nml = generate_all_l(n, m - 1, l)\n",
        "\n",
        "n = 34\n",
        "max_value_discard_list, discard_state_nml = states_to_max_value_list(state_nml, hand_nml, value_hand_nml, n, m, l)\n",
        "#for i in max_value_discard_list: print(i) \n",
        "discard_hist_nml = states_to_hist(discard_state_nml, n)\n",
        "discard_ans_vector_nml = np.array(discard_ans_prob_vector(max_value_discard_list, n, m, l))\n",
        "#print(discard_ans_vector_nml[:5])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n",
            "iteration\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkQ8lU3qZ1Hf",
        "colab_type": "text"
      },
      "source": [
        " ### Policy Networkに必要な関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bqx4TfAYx4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrintDot(keras.callbacks.Callback):\n",
        "    def __init__(self, epochs):\n",
        "        self.epochs = epochs\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if epoch % (self.epochs // 5) == 0: print(logs.get('loss'))\n",
        "        if epoch % 10 == 0: print('.', end='')\n",
        "\n",
        "# discard_state_nmlをone_hot化する関数\n",
        "def one_hot(discard_state_nml, num):\n",
        "    if num == 1:\n",
        "        return one_hot_vector1(discard_state_nml, n)\n",
        "    elif num == 2:\n",
        "        discard_hist_nml = states_to_hist(discard_state_nml, n)\n",
        "        return one_hot_vector2(discard_hist_nml, n, l)\n",
        "    else:\n",
        "        discard_hist_nml = states_to_hist(discard_state_nml, n)\n",
        "        return one_hot_vector3(discard_hist_nml, n, l)\n",
        "\n",
        "# predictionsから求めた捨て牌のリストを返す関数\n",
        "def make_pred_arg_list_split(predictions, discard_state_test):\n",
        "    pred_arg_list = []\n",
        "    for i  in range(len(predictions)):\n",
        "        tile = np.argmax(predictions[i])\n",
        "        if tile not in discard_state_test[i]:\n",
        "            max_val = 0\n",
        "            max_tile = 0\n",
        "            for t in discard_state_test[i]:\n",
        "                if predictions[i][t] > max_val:\n",
        "                    max_val = predictions[i][t]\n",
        "                    max_tile = t\n",
        "            tile = max_tile\n",
        "        pred_arg_list.append(tile)\n",
        "    return pred_arg_list\n",
        "\n",
        "# 正解率を返す関数\n",
        "def acc_score_split(pred_arg_list, discard_state_test): \n",
        "    tr_count = 0\n",
        "    fal_count = 0\n",
        "    for i, pred_arg in enumerate(pred_arg_list):\n",
        "        #print(i, j)\n",
        "        for state, discard_set in max_value_discard_list:\n",
        "            if discard_state_test[i] == state:\n",
        "                if pred_arg in discard_set:\n",
        "                    tr_count += 1\n",
        "                else:\n",
        "                    fal_count += 1\n",
        "    print('true count {}  false count {}'.format(tr_count, fal_count))\n",
        "    print('accuracy rate', tr_count / (tr_count + fal_count))\n",
        "    return tr_count / (tr_count + fal_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD74ZoQ1zRLi",
        "colab_type": "text"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiRPZouHaheP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6cbce37f-b317-4a4a-b985-d9a4ad748c04"
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 8  # orig paper trained all networks with batch_size=128\n",
        "EPOCHS = 3000\n",
        "num_classes = n\n",
        "num = 3\n",
        "\n",
        "depth_num = 2\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "depth = depth_num * 6 + 2\n",
        "\n",
        "# Model name, depth and version(Orig paper: version = 1 (ResNet v1))\n",
        "model_type = 'ResNet%d_v1' % (depth)\n",
        "print(model_type)\n",
        "\n",
        "discard_state_train, discard_state_test, discard_ans_vector_train, discard_ans_vector_test = train_test_split(discard_state_nml, discard_ans_vector_nml, test_size=0.25)\n",
        "one_hot_discard_state_train = one_hot(discard_state_train, num).reshape(len(discard_state_train), n, l, 1)\n",
        "one_hot_discard_state_test = one_hot(discard_state_test, num).reshape(len(discard_state_test), n, l, 1)\n",
        "#print(discard_ans_vector_test[:5])\n",
        "\n",
        "# Input image dimensions.\n",
        "input_shape = one_hot_discard_state_train.shape[1:]\n",
        "# if input_shape != (n, l, 1):\n",
        "#     raise Exception\n",
        "\n",
        "print('one_hot_discard_state_train shape(x_train shape):', one_hot_discard_state_train.shape)\n",
        "print(one_hot_discard_state_train.shape[0], 'train samples')\n",
        "print(one_hot_discard_state_test.shape[0], 'test samples')\n",
        "print('discard_ans_vector_train shape(y_train shape):', discard_ans_vector_train.shape)\n",
        "\n",
        "\n",
        "def resnet_layer(inputs, num_filters=16, kernel_size=3, strides=1, activation='relu', batch_normalization=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))\n",
        "    \n",
        "    x = conv(inputs)\n",
        "    if batch_normalization:\n",
        "        x = BatchNormalization()(x)\n",
        "    if activation is not None:\n",
        "        x = Activation(activation)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=n):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (cifar10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x, num_filters=num_filters, strides=strides)\n",
        "            y = resnet_layer(inputs=y, num_filters=num_filters, activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match changed dims\n",
        "                x = resnet_layer(inputs=x, num_filters=num_filters, kernel_size=1, strides=strides, activation=None, batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    # x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax', kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy', 'categorical_accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Run training\n",
        "plt.figure()\n",
        "start_time = time.time()\n",
        "history = model.fit(one_hot_discard_state_train, discard_ans_vector_train,\n",
        "                  batch_size=batch_size,\n",
        "                  epochs=EPOCHS,\n",
        "                  callbacks=[PrintDot(EPOCHS)],\n",
        "                  validation_data=(one_hot_discard_state_test, discard_ans_vector_test),\n",
        "                  shuffle=True,\n",
        "                  verbose=0)\n",
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "print('\\n',hist.tail())\n",
        "\n",
        "plt.plot(hist['epoch'], hist['val_acc'])\n",
        "plt.plot(hist['epoch'], hist['categorical_accuracy'])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('categorical accuracy')\n",
        "plt.legend()\n",
        "\n",
        "print(round(time.time() - start_time, 1))\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(one_hot_discard_state_test, discard_ans_vector_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet14_v1\n",
            "one_hot_discard_state_train shape(x_train shape): (857, 34, 4, 1)\n",
            "857 train samples\n",
            "286 test samples\n",
            "discard_ans_vector_train shape(y_train shape): (857, 34)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_31 (InputLayer)           (None, 34, 4, 1)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_447 (Conv2D)             (None, 34, 4, 16)    160         input_31[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_389 (BatchN (None, 34, 4, 16)    64          conv2d_447[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_389 (Activation)     (None, 34, 4, 16)    0           batch_normalization_389[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_448 (Conv2D)             (None, 34, 4, 16)    2320        activation_389[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_390 (BatchN (None, 34, 4, 16)    64          conv2d_448[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_390 (Activation)     (None, 34, 4, 16)    0           batch_normalization_390[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_449 (Conv2D)             (None, 34, 4, 16)    2320        activation_390[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_391 (BatchN (None, 34, 4, 16)    64          conv2d_449[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_181 (Add)                   (None, 34, 4, 16)    0           activation_389[0][0]             \n",
            "                                                                 batch_normalization_391[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_391 (Activation)     (None, 34, 4, 16)    0           add_181[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_450 (Conv2D)             (None, 34, 4, 16)    2320        activation_391[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_392 (BatchN (None, 34, 4, 16)    64          conv2d_450[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_392 (Activation)     (None, 34, 4, 16)    0           batch_normalization_392[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_451 (Conv2D)             (None, 34, 4, 16)    2320        activation_392[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_393 (BatchN (None, 34, 4, 16)    64          conv2d_451[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_182 (Add)                   (None, 34, 4, 16)    0           activation_391[0][0]             \n",
            "                                                                 batch_normalization_393[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_393 (Activation)     (None, 34, 4, 16)    0           add_182[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_452 (Conv2D)             (None, 17, 2, 32)    4640        activation_393[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_394 (BatchN (None, 17, 2, 32)    128         conv2d_452[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_394 (Activation)     (None, 17, 2, 32)    0           batch_normalization_394[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_453 (Conv2D)             (None, 17, 2, 32)    9248        activation_394[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_454 (Conv2D)             (None, 17, 2, 32)    544         activation_393[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_395 (BatchN (None, 17, 2, 32)    128         conv2d_453[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_183 (Add)                   (None, 17, 2, 32)    0           conv2d_454[0][0]                 \n",
            "                                                                 batch_normalization_395[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_395 (Activation)     (None, 17, 2, 32)    0           add_183[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_455 (Conv2D)             (None, 17, 2, 32)    9248        activation_395[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_396 (BatchN (None, 17, 2, 32)    128         conv2d_455[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_396 (Activation)     (None, 17, 2, 32)    0           batch_normalization_396[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_456 (Conv2D)             (None, 17, 2, 32)    9248        activation_396[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_397 (BatchN (None, 17, 2, 32)    128         conv2d_456[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_184 (Add)                   (None, 17, 2, 32)    0           activation_395[0][0]             \n",
            "                                                                 batch_normalization_397[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_397 (Activation)     (None, 17, 2, 32)    0           add_184[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_457 (Conv2D)             (None, 9, 1, 64)     18496       activation_397[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_398 (BatchN (None, 9, 1, 64)     256         conv2d_457[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_398 (Activation)     (None, 9, 1, 64)     0           batch_normalization_398[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_458 (Conv2D)             (None, 9, 1, 64)     36928       activation_398[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_459 (Conv2D)             (None, 9, 1, 64)     2112        activation_397[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_399 (BatchN (None, 9, 1, 64)     256         conv2d_458[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_185 (Add)                   (None, 9, 1, 64)     0           conv2d_459[0][0]                 \n",
            "                                                                 batch_normalization_399[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_399 (Activation)     (None, 9, 1, 64)     0           add_185[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_460 (Conv2D)             (None, 9, 1, 64)     36928       activation_399[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_400 (BatchN (None, 9, 1, 64)     256         conv2d_460[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_400 (Activation)     (None, 9, 1, 64)     0           batch_normalization_400[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_461 (Conv2D)             (None, 9, 1, 64)     36928       activation_400[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_401 (BatchN (None, 9, 1, 64)     256         conv2d_461[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_186 (Add)                   (None, 9, 1, 64)     0           activation_399[0][0]             \n",
            "                                                                 batch_normalization_401[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_401 (Activation)     (None, 9, 1, 64)     0           add_186[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_28 (Flatten)            (None, 576)          0           activation_401[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 34)           19618       flatten_28[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 195,234\n",
            "Trainable params: 194,306\n",
            "Non-trainable params: 928\n",
            "__________________________________________________________________________________________________\n",
            "1.8715440086333588\n",
            "............................................................0.049377479939314003\n",
            "............................................................0.03913090669396774\n",
            "............................................................0.03668028757828119\n",
            "............................................................0.032947023030774736\n",
            "............................................................\n",
            "       val_loss   val_acc  ...  categorical_accuracy  epoch\n",
            "2995  1.128974  0.832168  ...              0.961494   2995\n",
            "2996  0.990016  0.846154  ...              0.981330   2996\n",
            "2997  1.010687  0.846154  ...              0.982497   2997\n",
            "2998  0.976429  0.863636  ...              0.982497   2998\n",
            "2999  0.989825  0.853147  ...              0.985998   2999\n",
            "\n",
            "[5 rows x 7 columns]\n",
            "8213.0\n",
            "286/286 [==============================] - 0s 291us/step\n",
            "Test loss: 0.9898250348918087\n",
            "Test accuracy: 0.8531468535636688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ns5IHBTMzK3lQMDOz0v8AQGMTfw38KQ4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcCipW4szT8k",
        "colab_type": "text"
      },
      "source": [
        "### train2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6_gJoaadibR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "494d03b3-6120-427d-c357-5ce535dc3abf"
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 8  # orig paper trained all networks with batch_size=128\n",
        "EPOCHS = 3000\n",
        "num_classes = n\n",
        "num = 3\n",
        "\n",
        "depth_num = 2\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "depth = depth_num * 6 + 2\n",
        "\n",
        "# Model name, depth and version(Orig paper: version = 1 (ResNet v1))\n",
        "model_type = 'ResNet%d_v1' % (depth)\n",
        "print(model_type)\n",
        "\n",
        "def resnet_layer(inputs, num_filters=16, kernel_size=3, strides=1, activation='relu', batch_normalization=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters, kernel_size=kernel_size, strides=strides, padding='same', kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = conv(inputs)\n",
        "    if batch_normalization:\n",
        "        x = BatchNormalization()(x)\n",
        "    if activation is not None:\n",
        "        x = Activation(activation)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth, num_classes=n):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (cifar10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x, num_filters=num_filters, strides=strides)\n",
        "            y = resnet_layer(inputs=y, num_filters=num_filters, activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match changed dims\n",
        "                x = resnet_layer(inputs=x, num_filters=num_filters, kernel_size=1, strides=strides, activation=None, batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    # x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes, activation='softmax', kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "repeat_num = 3\n",
        "plt.figure()\n",
        "sum_acc_result = pd.Series([0] * EPOCHS)\n",
        "sum_val_acc_result = pd.Series([0] * EPOCHS)\n",
        "sum_test_loss = []\n",
        "sum_test_acc = []\n",
        "for i in range(repeat_num):\n",
        "\n",
        "    discard_state_train, discard_state_test, discard_ans_vector_train, discard_ans_vector_test = train_test_split(discard_state_nml, discard_ans_vector_nml, test_size=0.25)\n",
        "    one_hot_discard_state_train = one_hot(discard_state_train, num).reshape(len(discard_state_train), n, l, 1)\n",
        "    one_hot_discard_state_test = one_hot(discard_state_test, num).reshape(len(discard_state_test), n, l, 1)\n",
        "    #print(discard_ans_vector_test[:5])\n",
        "\n",
        "    # Input image dimensions.\n",
        "    input_shape = one_hot_discard_state_train.shape[1:]\n",
        "\n",
        "    if i ==0:\n",
        "        print('one_hot_discard_state_train shape(x_train shape):', one_hot_discard_state_train.shape)\n",
        "        print(one_hot_discard_state_train.shape[0], 'train samples')\n",
        "        print(one_hot_discard_state_test.shape[0], 'test samples')\n",
        "        print('discard_ans_vector_train shape(y_train shape):', discard_ans_vector_train.shape)\n",
        "\n",
        "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "    if i == 0:\n",
        "        model.summary()\n",
        "\n",
        "    # Run training\n",
        "    start_time = time.time()\n",
        "    history = model.fit(one_hot_discard_state_train, discard_ans_vector_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=EPOCHS,\n",
        "                      callbacks=[PrintDot(EPOCHS)],\n",
        "                      validation_data=(one_hot_discard_state_test, discard_ans_vector_test),\n",
        "                      shuffle=True,\n",
        "                      verbose=0)\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    hist['epoch'] = history.epoch\n",
        "    print('\\n',hist[-5:])\n",
        "    \n",
        "    sum_acc_result += hist['acc']\n",
        "    sum_val_acc_result += hist['val_acc']\n",
        "\n",
        "    print('time(sec)', round(time.time() - start_time, 1))\n",
        "\n",
        "    # Score trained model.\n",
        "    scores = model.evaluate(one_hot_discard_state_test, discard_ans_vector_test, verbose=1)\n",
        "    print('Test loss:', scores[0])\n",
        "    print('Test accuracy:', scores[1])\n",
        "    sum_test_loss.append(scores[0])\n",
        "    sum_test_acc.append(scores[1])\n",
        "    print()\n",
        "   \n",
        "plt.plot(hist['epoch'], sum_acc_result / 3, label='categorical acc')\n",
        "plt.plot(hist['epoch'], sum_val_acc_result / 3, label='valid acc')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "\n",
        "print('test_loss(mean)', sum(sum_test_loss) / 3)\n",
        "print('test_accuracy(mean)', sum(sum_test_acc) / 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet14_v1\n",
            "one_hot_discard_state_train shape(x_train shape): (857, 34, 4, 1)\n",
            "857 train samples\n",
            "286 test samples\n",
            "discard_ans_vector_train shape(y_train shape): (857, 34)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_45 (InputLayer)           (None, 34, 4, 1)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_657 (Conv2D)             (None, 34, 4, 16)    160         input_45[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_571 (BatchN (None, 34, 4, 16)    64          conv2d_657[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_571 (Activation)     (None, 34, 4, 16)    0           batch_normalization_571[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_658 (Conv2D)             (None, 34, 4, 16)    2320        activation_571[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_572 (BatchN (None, 34, 4, 16)    64          conv2d_658[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_572 (Activation)     (None, 34, 4, 16)    0           batch_normalization_572[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_659 (Conv2D)             (None, 34, 4, 16)    2320        activation_572[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_573 (BatchN (None, 34, 4, 16)    64          conv2d_659[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_265 (Add)                   (None, 34, 4, 16)    0           activation_571[0][0]             \n",
            "                                                                 batch_normalization_573[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_573 (Activation)     (None, 34, 4, 16)    0           add_265[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_660 (Conv2D)             (None, 34, 4, 16)    2320        activation_573[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_574 (BatchN (None, 34, 4, 16)    64          conv2d_660[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_574 (Activation)     (None, 34, 4, 16)    0           batch_normalization_574[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_661 (Conv2D)             (None, 34, 4, 16)    2320        activation_574[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_575 (BatchN (None, 34, 4, 16)    64          conv2d_661[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_266 (Add)                   (None, 34, 4, 16)    0           activation_573[0][0]             \n",
            "                                                                 batch_normalization_575[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_575 (Activation)     (None, 34, 4, 16)    0           add_266[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_662 (Conv2D)             (None, 17, 2, 32)    4640        activation_575[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_576 (BatchN (None, 17, 2, 32)    128         conv2d_662[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_576 (Activation)     (None, 17, 2, 32)    0           batch_normalization_576[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_663 (Conv2D)             (None, 17, 2, 32)    9248        activation_576[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_664 (Conv2D)             (None, 17, 2, 32)    544         activation_575[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_577 (BatchN (None, 17, 2, 32)    128         conv2d_663[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_267 (Add)                   (None, 17, 2, 32)    0           conv2d_664[0][0]                 \n",
            "                                                                 batch_normalization_577[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_577 (Activation)     (None, 17, 2, 32)    0           add_267[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_665 (Conv2D)             (None, 17, 2, 32)    9248        activation_577[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_578 (BatchN (None, 17, 2, 32)    128         conv2d_665[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_578 (Activation)     (None, 17, 2, 32)    0           batch_normalization_578[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_666 (Conv2D)             (None, 17, 2, 32)    9248        activation_578[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_579 (BatchN (None, 17, 2, 32)    128         conv2d_666[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_268 (Add)                   (None, 17, 2, 32)    0           activation_577[0][0]             \n",
            "                                                                 batch_normalization_579[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_579 (Activation)     (None, 17, 2, 32)    0           add_268[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_667 (Conv2D)             (None, 9, 1, 64)     18496       activation_579[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_580 (BatchN (None, 9, 1, 64)     256         conv2d_667[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_580 (Activation)     (None, 9, 1, 64)     0           batch_normalization_580[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_668 (Conv2D)             (None, 9, 1, 64)     36928       activation_580[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_669 (Conv2D)             (None, 9, 1, 64)     2112        activation_579[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_581 (BatchN (None, 9, 1, 64)     256         conv2d_668[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_269 (Add)                   (None, 9, 1, 64)     0           conv2d_669[0][0]                 \n",
            "                                                                 batch_normalization_581[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_581 (Activation)     (None, 9, 1, 64)     0           add_269[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_670 (Conv2D)             (None, 9, 1, 64)     36928       activation_581[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_582 (BatchN (None, 9, 1, 64)     256         conv2d_670[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_582 (Activation)     (None, 9, 1, 64)     0           batch_normalization_582[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_671 (Conv2D)             (None, 9, 1, 64)     36928       activation_582[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_583 (BatchN (None, 9, 1, 64)     256         conv2d_671[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_270 (Add)                   (None, 9, 1, 64)     0           activation_581[0][0]             \n",
            "                                                                 batch_normalization_583[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_583 (Activation)     (None, 9, 1, 64)     0           add_270[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_42 (Flatten)            (None, 576)          0           activation_583[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_42 (Dense)                (None, 34)           19618       flatten_42[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 195,234\n",
            "Trainable params: 194,306\n",
            "Non-trainable params: 928\n",
            "__________________________________________________________________________________________________\n",
            "1.9352752139646225\n",
            "............................................................0.052126384380435445\n",
            "............................................................0.046637372866864105\n",
            "............................................................0.034702353031660325\n",
            "............................................................0.03327483556025782\n",
            "............................................................\n",
            "       val_loss   val_acc      loss       acc  epoch\n",
            "2995  1.408163  0.807692  0.140575  0.953326   2995\n",
            "2996  1.186089  0.811189  0.098400  0.974329   2996\n",
            "2997  1.239497  0.835664  0.068661  0.973162   2997\n",
            "2998  1.275434  0.825175  0.037682  0.984831   2998\n",
            "2999  1.272899  0.825175  0.033648  0.988331   2999\n",
            "time(sec) 9410.6\n",
            "286/286 [==============================] - 0s 329us/step\n",
            "Test loss: 1.2728991933635898\n",
            "Test accuracy: 0.8251748230907466\n",
            "\n",
            "1.8909309735236952\n",
            "............................................................0.045580798818739596\n",
            "............................................................0.03862752868221201\n",
            "............................................................0.034818147460179125\n",
            "............................................................0.031662360268407\n",
            "............................................................\n",
            "       val_loss   val_acc      loss       acc  epoch\n",
            "2995  1.088817  0.867133  0.030858  0.991832   2995\n",
            "2996  1.072447  0.870629  0.030289  0.984831   2996\n",
            "2997  1.070966  0.867133  0.030446  0.988331   2997\n",
            "2998  1.103952  0.853147  0.031076  0.984831   2998\n",
            "2999  1.020998  0.867133  0.032293  0.985998   2999\n",
            "time(sec) 9574.7\n",
            "286/286 [==============================] - 0s 376us/step\n",
            "Test loss: 1.020997560941256\n",
            "Test accuracy: 0.8671328654656043\n",
            "\n",
            "2.0137179240045358\n",
            "............................................................0.04911502641533331\n",
            ".........................."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euom05ITYyFH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}