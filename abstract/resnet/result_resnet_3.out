iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
ResNet14_v1
one_hot_discard_state_train shape (x_train shape): (7791, 34, 4, 1)
7791 train samples
2598 test samples
discard_ans_vector_train shape (y_train shape): (7791, 34)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 34, 4, 1)     0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 34, 4, 16)    160         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_v1 (BatchNo (None, 34, 4, 16)    64          conv2d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 34, 4, 16)    0           batch_normalization_v1[0][0]     
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 34, 4, 16)    2320        activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_v1_1 (Batch (None, 34, 4, 16)    64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 34, 4, 16)    0           batch_normalization_v1_1[0][0]   
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 34, 4, 16)    2320        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_v1_2 (Batch (None, 34, 4, 16)    64          conv2d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 34, 4, 16)    0           activation[0][0]                 
                                                                 batch_normalization_v1_2[0][0]   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 34, 4, 16)    0           add[0][0]                        
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 34, 4, 16)    2320        activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_v1_3 (Batch (None, 34, 4, 16)    64          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 34, 4, 16)    0           batch_normalization_v1_3[0][0]   
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 34, 4, 16)    2320        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_v1_4 (Batch (None, 34, 4, 16)    64          conv2d_4[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 34, 4, 16)    0           activation_2[0][0]               
                                                                 batch_normalization_v1_4[0][0]   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 34, 4, 16)    0           add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 17, 2, 32)    4640        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_v1_5 (Batch (None, 17, 2, 32)    128         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 17, 2, 32)    0           batch_normalization_v1_5[0][0]   
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 17, 2, 32)    9248        activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 17, 2, 32)    544         activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_v1_6 (Batch (None, 17, 2, 32)    128         conv2d_6[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 17, 2, 32)    0           conv2d_7[0][0]                   
                                                                 batch_normalization_v1_6[0][0]   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 17, 2, 32)    0           add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 17, 2, 32)    9248        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_v1_7 (Batch (None, 17, 2, 32)    128         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 17, 2, 32)    0           batch_normalization_v1_7[0][0]   
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 17, 2, 32)    9248        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_v1_8 (Batch (None, 17, 2, 32)    128         conv2d_9[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 17, 2, 32)    0           activation_6[0][0]               
                                                                 batch_normalization_v1_8[0][0]   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 17, 2, 32)    0           add_3[0][0]                      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 9, 1, 64)     18496       activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_v1_9 (Batch (None, 9, 1, 64)     256         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 9, 1, 64)     0           batch_normalization_v1_9[0][0]   
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 9, 1, 64)     36928       activation_9[0][0]               
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 9, 1, 64)     2112        activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_v1_10 (Batc (None, 9, 1, 64)     256         conv2d_11[0][0]                  
__________________________________________________________________________________________________
add_4 (Add)                     (None, 9, 1, 64)     0           conv2d_12[0][0]                  
                                                                 batch_normalization_v1_10[0][0]  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 9, 1, 64)     0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 9, 1, 64)     36928       activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_v1_11 (Batc (None, 9, 1, 64)     256         conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 9, 1, 64)     0           batch_normalization_v1_11[0][0]  
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 9, 1, 64)     36928       activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_v1_12 (Batc (None, 9, 1, 64)     256         conv2d_14[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 9, 1, 64)     0           activation_10[0][0]              
                                                                 batch_normalization_v1_12[0][0]  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 9, 1, 64)     0           add_5[0][0]                      
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 9, 1, 2)      1154        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_v1_13 (Batc (None, 9, 1, 2)      8           conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 9, 1, 2)      0           batch_normalization_v1_13[0][0]  
__________________________________________________________________________________________________
flatten (Flatten)               (None, 18)           0           activation_13[0][0]              
__________________________________________________________________________________________________
dense (Dense)                   (None, 34)           646         flatten[0][0]                    
==================================================================================================
Total params: 177,424
Trainable params: 176,492
Non-trainable params: 932
__________________________________________________________________________________________________
1.58364
............................................................0.146704
............................................................0.131461
............................................................0.139659
............................................................0.12609
............................................................
           loss       acc  val_loss   val_acc  epoch
2995  0.131412  0.977410  0.723875  0.847960   2995
2996  0.133108  0.975613  0.688100  0.858737   2996
2997  0.122016  0.980105  0.794853  0.859122   2997
2998  0.134672  0.977795  0.710421  0.858353   2998
2999  0.122261  0.979078  0.689872  0.867975   2999
time(sec) 44015.4
  32/2598 [..............................] - ETA: 0s - loss: 0.3353 - acc: 0.9062 384/2598 [===>..........................] - ETA: 0s - loss: 0.7619 - acc: 0.8698 736/2598 [=======>......................] - ETA: 0s - loss: 0.6940 - acc: 0.87361088/2598 [===========>..................] - ETA: 0s - loss: 0.7321 - acc: 0.86121440/2598 [===============>..............] - ETA: 0s - loss: 0.7437 - acc: 0.85901792/2598 [===================>..........] - ETA: 0s - loss: 0.7141 - acc: 0.86612144/2598 [=======================>......] - ETA: 0s - loss: 0.7119 - acc: 0.86572496/2598 [===========================>..] - ETA: 0s - loss: 0.6970 - acc: 0.86742598/2598 [==============================] - 0s 151us/sample - loss: 0.6899 - acc: 0.8680
Test loss: 0.68987
Test accuracy: 0.86798

1.598194
............................................................0.16183
............................................................0.152004
............................................................0.134194
............................................................0.12641
............................................................
           loss       acc  val_loss   val_acc  epoch
2995  0.122925  0.979463  0.647731  0.864126   2995
2996  0.122567  0.977410  0.716705  0.852579   2996
2997  0.134015  0.974458  0.683735  0.858353   2997
2998  0.124682  0.977795  0.677035  0.870670   2998
2999  0.125965  0.978565  0.640208  0.870670   2999
time(sec) 44600.7
  32/2598 [..............................] - ETA: 0s - loss: 0.6236 - acc: 0.9062 352/2598 [===>..........................] - ETA: 0s - loss: 0.6595 - acc: 0.8807 672/2598 [======>.......................] - ETA: 0s - loss: 0.6508 - acc: 0.8795 992/2598 [==========>...................] - ETA: 0s - loss: 0.6501 - acc: 0.87401344/2598 [==============>...............] - ETA: 0s - loss: 0.6669 - acc: 0.86831696/2598 [==================>...........] - ETA: 0s - loss: 0.6943 - acc: 0.85972048/2598 [======================>.......] - ETA: 0s - loss: 0.6675 - acc: 0.86772400/2598 [==========================>...] - ETA: 0s - loss: 0.6571 - acc: 0.86922598/2598 [==============================] - 0s 158us/sample - loss: 0.6402 - acc: 0.8707
Test loss: 0.64021
Test accuracy: 0.87067

1.714212
............................................................0.149365
............................................................0.142703
............................................................0.128321
............................................................0.136243
............................................................
           loss       acc  val_loss   val_acc  epoch
2995  0.149571  0.974201  0.676074  0.858353   2995
2996  0.122610  0.981517  0.690283  0.860662   2996
2997  0.120323  0.982159  0.667997  0.857583   2997
2998  0.134515  0.977153  0.676496  0.857968   2998
2999  0.117507  0.980234  0.745475  0.857583   2999
time(sec) 45259.3
  32/2598 [..............................] - ETA: 0s - loss: 1.0833 - acc: 0.9062 352/2598 [===>..........................] - ETA: 0s - loss: 0.8503 - acc: 0.8523 672/2598 [======>.......................] - ETA: 0s - loss: 0.7744 - acc: 0.8676 992/2598 [==========>...................] - ETA: 0s - loss: 0.7612 - acc: 0.85791312/2598 [==============>...............] - ETA: 0s - loss: 0.7363 - acc: 0.86131632/2598 [=================>............] - ETA: 0s - loss: 0.7353 - acc: 0.86211952/2598 [=====================>........] - ETA: 0s - loss: 0.7473 - acc: 0.85962272/2598 [=========================>....] - ETA: 0s - loss: 0.7756 - acc: 0.85432592/2598 [============================>.] - ETA: 0s - loss: 0.7457 - acc: 0.85802598/2598 [==============================] - 0s 163us/sample - loss: 0.7455 - acc: 0.8576
Test loss: 0.74547
Test accuracy: 0.85758


Test_loss(mean) 0.69185
Test_accuracy(mean) 0.86541
