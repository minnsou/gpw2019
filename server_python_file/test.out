iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
iteration
ResNet8_v1
one_hot_discard_state_train shape (x_train shape): (857, 34, 4, 1)
857 train samples
286 test samples
discard_ans_vector_train shape (y_train shape): (857, 34)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 34, 4, 1)     0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 34, 4, 16)    160         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 34, 4, 16)    64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 34, 4, 16)    0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 34, 4, 16)    2320        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 34, 4, 16)    64          conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 34, 4, 16)    0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 34, 4, 16)    2320        activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 34, 4, 16)    64          conv2d_3[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 34, 4, 16)    0           activation_1[0][0]               
                                                                 batch_normalization_3[0][0]      
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 34, 4, 16)    0           add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 17, 2, 32)    4640        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 17, 2, 32)    128         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 17, 2, 32)    0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 17, 2, 32)    9248        activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 17, 2, 32)    544         activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 17, 2, 32)    128         conv2d_5[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 17, 2, 32)    0           conv2d_6[0][0]                   
                                                                 batch_normalization_5[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 17, 2, 32)    0           add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 9, 1, 64)     18496       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 9, 1, 64)     256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 9, 1, 64)     0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 9, 1, 64)     36928       activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 9, 1, 64)     2112        activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 9, 1, 64)     256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 9, 1, 64)     0           conv2d_9[0][0]                   
                                                                 batch_normalization_7[0][0]      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 9, 1, 64)     0           add_3[0][0]                      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 9, 1, 2)      1154        activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 9, 1, 2)      8           conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 9, 1, 2)      0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 18)           0           activation_8[0][0]               
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 34)           646         flatten_1[0][0]                  
==================================================================================================
Total params: 79,536
Trainable params: 79,052
Non-trainable params: 484
__________________________________________________________________________________________________
2.569902
.0.265623
.0.170531
.0.098194
.0.091574
.
     val_loss   val_acc      loss       acc  epoch
45  0.820497  0.800699  0.107235  0.985998     45
46  0.911196  0.779720  0.116220  0.984831     46
47  1.202751  0.744755  0.243150  0.939323     47
48  0.929962  0.779720  0.245080  0.940490     48
49  0.757451  0.793706  0.171804  0.962660     49
time(sec) 55.0

 32/286 [==>...........................] - ETA: 0s
286/286 [==============================] - 0s 128us/step
Test loss: 0.75745
Test accuracy: 0.79371

2.755363
.0.329926
.0.1578
.0.229387
.0.154167
.
     val_loss   val_acc      loss       acc  epoch
45  0.618736  0.825175  0.206965  0.952159     45
46  0.602914  0.811189  0.125068  0.982497     46
47  0.559055  0.832168  0.113256  0.982497     47
48  0.571769  0.818182  0.103310  0.981330     48
49  0.568373  0.821678  0.147232  0.974329     49
time(sec) 57.0

 32/286 [==>...........................] - ETA: 0s
286/286 [==============================] - 0s 133us/step
Test loss: 0.56837
Test accuracy: 0.82168

2.88925
.0.293429
.0.144119
.0.10686
.0.102781
.
     val_loss   val_acc      loss       acc  epoch
45  0.627185  0.867133  0.105019  0.978996     45
46  0.772419  0.835664  0.230218  0.945158     46
47  0.616265  0.881119  0.165064  0.966161     47
48  0.624680  0.863636  0.115030  0.975496     48
49  0.624514  0.860140  0.104568  0.987165     49
time(sec) 57.6

 32/286 [==>...........................] - ETA: 0s
286/286 [==============================] - 0s 141us/step
Test loss: 0.62451
Test accuracy: 0.86014


Test_loss(mean) 0.65011
Test_accuracy(mean) 0.82517
